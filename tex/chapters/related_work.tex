\chapter{Related Work}
\label{cha:related-work}

\begin{comment}
What other research has been conducted in this area and how is it related to your work?
This section is thus where your literature review will be presented. It is important when presenting the review
that you give an overview of the motivating elements of the work going on in your field and how these relate to your work,
rather than a list of contributors and what they have done.
This means that you need to extract the key important factors for your work and discuss how others have addressed
each of these factors and what the advantages/disadvantages are with such approaches.
As you mention other authors, you should reference their work.
Note that the reference list reflects the literature you have read {\em and\/} have cited.
This will only be a subset of the literature that you have read.

A good way to find relevant work is by checking what others are referencing, e.g., in papers you have already found
However, when doing that,
do not fall into one of the common traps, such as re-iterating someone's false quote or faulty analysis of
a previous paper (check the original source!), or getting stuck inside a local research cluster (a group of
researchers that mainly refer to the ones using the same type of approaches or similar ideas).

Make sure that it is clear how and why you decided to include some references (and discard others). As in all parts of research, it should ideally be possible for someone else to reproduce your work, also when it comes to finding the relevant references.
There are (at least) three basic methods for finding references:
\begin{enumerate}
    \item Trust the authorities (e.g., your supervisor) to dig out good texts for you.
          Those can often be used as a seed set for:
    \item Snowballing, where you have some good articles and check the references in them for other good ones.
          Note that this can be done both backwards and forwards on the timeline; that is, using tools like Google Scholar, you can also check who refers \textit{to\/} the good articles you have already found.
\end{enumerate}

Note that a reference needs to be complete: you should always give the full name of a conference or journal,
always include page numbers, always say where a book or thesis was published, and where a conference took place, as further described in Section~\ref{sec:reference_list}.
\end{comment}

The related works are divided into three sections, each being relevant to the project in different ways. \Autosectionref{sec:gis-with-llms} is the most obviously relevant section, discussing works in which \acrshortpl{acr:llm} were employed to perform tasks in the geospatial realm. \Autosectionref{sec:prompt-engineering-and-planning-strategies} delves into different prompt engineering and planning strategies that could be useful to make an autonomous \acrshort{acr:gis} agent perform better and more reliably. \Autosectionref{sec:retrieval-automented-generation} discusses \gls{acr:rag}, that is, how one can provide an autonomous \acrshort{acr:llm}-based agent with externaFcontributionsl tooling and up-to-date information. \cite{wengLLMPoweredAutonomous2023} provides a good summary of techniques relevant to \autoref{sec:prompt-engineering-and-planning-strategies} and \autoref{sec:retrieval-automented-generation}.



\section[GIS with LLMs]{\acrshort{acr:gis} with \acrshortpl{acr:llm}}\label{sec:gis-with-llms}

A substantial body of work have been done in recent years to assess the geospatial knowledge of \acrshortpl{acr:llm}, and how they can be fine-tuned or embedded into frameworks to serve downstream tasks.

\subsection{Taking the Temperature on GIS with LLMs on Social Media}

The search term \enquote{LLM GIS} on Twitter/X shows various ways that people are using \acrshortpl{acr:llm} for \acrshort{acr:gis}-related tasks. One user praises the use of ChatGPT to \enquote{extract and categorize data from unstructured text}, sharing a video from an ESRI conference\footnote{\url{https://twitter.com/mildthing99/status/1658507921234296833}}. Twitter user Zeke Hausfather shares the discovery that \enquote{\acrshort{acr:gpt}4 now supports processing netCDF files and other geospatial data, as well as some pretty amazing visualization}\footnote{\url{https://twitter.com/mildthing99/status/1658507921234296833}}. Arpit Gupta shares a summary of a paper on generative regulatory measurement on Twitter/X, where he explains how they have utilized \acrshortpl{acr:llm} to decode and interpret status updates and administrative documents, including, for instance, mapping zoning and housing regulations for the suburbs of Chicago\footnote{\url{https://twitter.com/arpitrage/status/1723033894801309893}}. Yu Zhao speculate in the effectiveness of smaller \acrshortpl{acr:llm} fine-tuned on domain-specific knowledge for \acrshort{acr:gis} or remote sensing\footnote{\url{https://twitter.com/zhaoyutim/status/1651233975946321920}}, an interest other users share\footnote{\url{https://twitter.com/zhaoyutim/status/1651233975946321920}}\footnote{\url{https://twitter.com/DougButdorf/status/1670938318979121152}}.

Swapping out \enquote{LLM} with \enquote{ChatGPT} gave more results. One user shows you using ChatGPT with tabular geographical data can increase productivity\footnote{\url{https://twitter.com/BooneLovesVideo/status/1617479222724857856}}. Other users show how they use ChatGPT for entertainment  or as an educational tool in a \acrshort{acr:gis} context\footnote{\url{https://twitter.com/briankingery87/status/1631365717269307394}}\footnote{\url{https://twitter.com/burdGIS/status/1614630141858316288}}\footnote{\url{https://twitter.com/_jsolly/status/1652867118797590528}}\footnote{\url{https://twitter.com/wanjohikibui/status/1628282272548806657}}\footnote{\url{https://twitter.com/GeoWithJustin/status/1641155652759199744}}. This appears to be the primary method by which people utilize ChatGPT, and it seems to offer mostly adequate responses. Another user highlights ChatGPT's built-in geographical context, using it to get GeoJSON polygons for a specified are directly\footnote{\url{https://twitter.com/at_dot_Py/status/1649985754800730112/}}.

On YouTube, the search term \enquote{ChatGPT GIS} yield a range of relevant responses. Several videos display how ChatGPT can be used to create Python code for \acrshort{acr:gis}-related purposes. Examples were found of users highlighting ChatGPT's abilities to generate Python code to manipulate geospatial files, perform analysis, and visualize, using Python libraries like GeoPandas and Folium\footnote{\url{https://www.youtube.com/watch?v=QDf-zc81NSE&t=1707s&ab_channel=GeoDeltaLabs}}\footnote{\url{https://www.youtube.com/watch?v=iNHQgLw7qZc&ab_channel=GeoDeltaLabs}}\footnote{\url{https://www.youtube.com/watch?v=BK2IzZZZC-k&ab_channel=MattForrest}}. Other users show how uploading geospatial files into ChatGPT using Code Interpreter can be an efficient workflow\footnote{\url{https://www.youtube.com/watch?v=dgzWLBYswh0&ab_channel=MiningGeologist}}. Some user demonstrate the QChatGPT\footnote{\url{https://plugins.qgis.org/plugins/QChatGPT/}} plugin to QGIS, which is a plugin integration between QGIS and the OpenAI \acrshort{acr:api}. QChatGPT does not seem to have any context of the current QGIS project the user is working on, but appears to have sparked some excitement among certain users, seeing how \acrshortpl{acr:llm} can assist them in their daily work as \acrshort{acr:gis} professionals\footnote{\url{https://www.youtube.com/watch?v=zUZs4GsDk6I&ab_channel=GISWorld}}\footnote{\url{https://www.youtube.com/watch?v=eEkVTUS8Qtc&ab_channel=HansvanderKwast}}\footnote{\url{https://www.youtube.com/watch?v=Tc-hHaDqoxY&ab_channel=DEVICKSGEOSPATIALCO.}}. One user shows an application with ChatGPT integration that can generate \acrshort{acr:sql} code and visualize geospatial data\footnote{\url{https://www.youtube.com/watch?v=gaA46aaWDuc&ab_channel=GeospatialWorld}}. A recurring user shows how one can use LangChain and its \acrshort{acr:sql} database plugins to \enquote{unlock ChatGPT's potential}\footnote{\url{https://www.youtube.com/watch?v=FoGm7d0paIo&t=1190s&ab_channel=MattForrest}}.

Lastly, the \enquote{FME Channel} released a recording of a webinar where they show how \acrshort{acr:gpt}-3 is being used in FME Data Integration Workflows\footnote{\url{https://www.youtube.com/watch?v=94ZDhgW8yMY&ab_channel=FMEChannel}}. They highlight how the OpenAI \acrshort{acr:api} allows for easy and automated no-code \acrshort{acr:api} to \acrshort{acr:api} workflows. On the FME Community website, an article writes about the \texttt{OpenAICompletionsConnector} and \texttt{OpenAIImageGenerator} transformers in FME\footnote{\url{https://community.safe.com/s/article/Tutorial-Getting-Started-with-OpenAI-in-FME}}. They list use cases such as, running data through the \acrshort{acr:ai} for analysis, generation of reports and summaries, generating scripts or \acrshort{acr:sql} for use in a data integration workflow, and automatic generation of images based on a dynamic input.



\subsection[]{Geospatial Context in \acrshortpl{acr:llm}}

\cite{scherrerHeLjuVarDial20202020} were able to show that \acrshort{acr:bert} can be fine-tuned to accurately predict geolocations from textual input, by winning a shared task on predicting geolocations from Twitter/Jodel messages in a workshop in 2020 \citep{gamanReportVarDialEvaluation2020}. By converting the task into a double regression problem, where they predicted latitude/longitude pairs from the output \texttt{[CLS]} representation of \acrshort{acr:bert} models. For a subtask on a Swiss Jodel dataset, they were able to achieve a median distance of 15.72 km from the ground truth, showing that \acrshortpl{acr:llm} can be trained correlate lingual features and geolocations.

\cite{robertsGPT4GEOHowLanguage2023} investigated extent of GPT-4's geospatial awareness through a set of case studies with increasing difficulties, starting with general factual tasks and finishing with complex questions such as generating country outlines and travel networks. The authors find that \acrshort{acr:gpt}-4 is \enquote{skilful at solving a variety of application-centric tasks}, almost having the ability to \enquote{see}, despite being a language model and thus only being able to interface with the world through sequenced, textual input. Examples include its ability to perform as a travel assistant in providing itinerary suggestions for a trip when provided with requirements, and its ability to provide start and end locations bird migrations generally correct, and in some cases highly accurate. While it becomes obvious that a lot of geospatial context have been embedded within the model during the vast pre-training, the question whether this is memorization or reasoning is a central one. The authors suggest that variability of tasks in their experiments deems it unlikely that it is all memorization, but they say that some things appear to be memorized.

\cite{mooneyUnderstandingGeospatialSkills2023} examined the performance of ChatGPT in a \acrfull{acr:gis} exam, aiming to assess its ability to grasp various geospatial concepts, highlighting its capabilities and limitations. Experiments were conducted on GPT-3.5 and GPT-4, which delivered performances equivalent to grades of D and B+, respectively. Additional experiments were conducted for more specialized areas of \acrshort{acr:gis}, including True/False questions about spatial analysis, and simple tasks in applied \acrshort{acr:gis} workflow. Experiments on the latter showed that ChatGPT-4 was able to correctly answer a relatively complex \acrshort{acr:gis} tasks involving seven different datasets, requiring seven steps in order to obtain a perfect score. Generally, ChatGPT-4 outperformed ChatGPT-3.5 in all tasks. While clearly powerful, the authors highlight a range of limitations, among which the multimodal nature of \acrshort{acr:gis}, which would hinder a straightforward application of existing models.

\cite{unluChatmapLargeLanguage2023} discussed importance of enabling \acrshortpl{acr:llm} to recognize and interpret geospatial data, and how \gls{acr:osm} can play an important role in offering \acrshortpl{acr:llm} linguistic access to vast cartographic datasets. He exemplifies this claim through a proof of concept in which he performs small-scale fine-tuning on an \acrshort{acr:llm} with 1B parameters, using an artificial supervised datasets curated by the more capable ChatGPT 3.5-turbo, which functions as a teacher model, generating prompt-answer pairs for given preprompts. The fine-tuned model displays promising ability of answer questions about a location's attributes, allowing the user to inquire about things like tourist appeal and potential profitability of businesses in the vicinity of the given location. \citeauthor{unluChatmapLargeLanguage2023} emphasizes the method's strengths for small datasets and minimal computational settings. The study also investigated the idea of using embeddings of the curated prepromts. Experimenting with average GLOVE embeddings, he showed that the latent structure of verbal descriptions of \gls{acr:osm} data can yield insightful patters. This, he argues, can prove useful when creating \acrfull{acr:rag} applications aimed at allowing users to retrieve geospatial information in a prompt-based manner.

\subsection[]{Autonomous \acrshort{acr:gis}}

\cite{liAutonomousGISNextgeneration2023} states that “autonomous \acrshort{acr:gis} will need to achieve five autonomous goals: self-generating, self-organizing, self-verifying, self-executing, and self-growing.”, and provide a “divide-and-conquer”-based method to address some of these goals. Furthermore, they propose a simple trial-and-error approach to addressing the self-verifying goal. They also highlight need of a memory system in a mature \gls{acr:llm}-based \gls{acr:gis} system, referring to the use of vector databases in autonomous agents like AutoGPT \citep{richardAutoGPTHeartOpensource2023}. Even with its shortages, the solution that \citep{liAutonomousGISNextgeneration2023} provide, called \acrshort{acr:llm}-Geo, is able to solve provide good solutions in various case studies by providing executable assemblies in a Python environment when provided with URLs to relevant data sets, along with a user-specified query.

\cite{zhangGeoGPTUnderstandingProcessing2023} use the LangChain framework \citep{chaseLangChain2022} in order to combine different GIS tools in a sequence in order to solve different sub-goals, and focuses on using the semantic understanding and reasoning abilities of \glspl{acr:llm} like (e.g., ChatGPT) to call externally defined tools, employing the \gls{acr:llm} as an agent or controller. The authors take great inspiration from the AutoGPT framework \citep{richardAutoGPTHeartOpensource2023}. The externally defined tools are described (manually) by its name and description. Said description contains information about the input parameters and output types of the tools/functions. Tools are defined for geospatial data collection, data processing and analysis, and data visualization. The effectiveness of the system is showcased in four case studies.

\cite{qiMaaSDBSpatialDatabases2023} discuss how \acrshortpl{acr:llm} can be used in spatial data management, facilitating a system that can learn from both structured and unstructured data, the latter of which is possibly the greatest strength of modern \acrshortpl{acr:llm}. They highlight the opportunity that \acrshortpl{acr:llm} provide in reducing the barrier to information retrieval for the general public, and discuss how these strengths can be used in spatial data management by leveraging a spatial database system trained from both structured and unstructured data, allowing for seamless access to spatial knowledge, also for those with little or no expertise in querying a spatial database. With that in mind, they envisage to use \textit{machine learning models as a spatial database} (MaaSDB), which when trained on structured and unstructured spatial data can \textit{generate query answers directly} instead of retrieving data from tables, the latter of which has been the most common way of using machine learning in database query processing. From conducting preliminary studies they present a system of \acrshort{acr:llm}-based system of query analysers, query plan generators, and a query result generators to handle natural language user queries. They propose a \gls{acr:gan}-based model to generate tabular data, seeing if such a model can remember the key characteristics of the data. Such a model will have a \textit{generator} $G$ that will produce a record a \textit{discriminator} $D$ that will classify whether the generated record resembles a real record. The results of this approach is promising, and further prompt-based test performed on ChatGPT demonstrates its potential to learn spatial knowledge and answer queries. While potentially powerful, they highlight a range of challenges of implementing their proposed system, such as hallucination, the limited availability of structured spatial data, generalizability issues, and the problem of updating the trained models when the underlying data changes.



\section{Prompt Engineering and Planning Strategies}\label{sec:prompt-engineering-and-planning-strategies}

\acrlongpl{acr:llm} have shown great abilities in problem-solving and decision-making tasks, but generally struggle as they are presented with larger and more complex tasks. Also, seeing as they are pure stochastic machines, the output is seldom reproducible. While the temperature parameter of the \acrshort{acr:gpt} models help serve as a control mechanism for this randomness, it does not guarantee fully predictable text generation. These issues have lead people into investigation \textit{prompt engineering} and various techniques for helping the models form plans when faced with large and complex tasks, both of which aim to guide the model into producing the desired response.

The \textit{Chain of Thought} strategy \citep{weiChainofThoughtPromptingElicits2023} aimed at complex reasoning in \acrlongpl{acr:llm} showed that reasoning can emerge naturally from sufficiently large \acrshortpl{acr:llm}.  \textit{Chain-of-Though prompting} entails the inclusion of examples of chain of thought sequences, that is, examples of how one might reason about a given problem in order to get to the answer, into the prompt. The exemplars are categorized into the types of tasks they aim to solve. This, along with instructing the model to think \enquote{step by step}, achieved a new state-of-the-art accuracy on the GSM8K benchmark of math word problems in early \citeyear{weiChainofThoughtPromptingElicits2023}.

The \textit{Tree of Thoughts} strategy \citep{yaoTreeThoughtsDeliberate2023} is a more recent planning strategy aimed at problem-solving with \acrlongpl{acr:llm}, and addresses a common limitation of \textit{vanilla} \acrshort{acr:llm} problem-solving, which often lacks the ability to explore strategically. Generalizing over \textit{Chain of Thought}, \textit{Tree of Thoughts} allows the \acrshort{acr:llm} to consider multiple different reasoning paths and to perform self-evaluation to decide the next course of action. \textit{Tree of Thoughts} can be used with different search algorithms. The authors discuss breadth-first search and depth-first search, and leave more advanced ones for future work. Using the \textit{Tree  of Thoughs} strategy proved very effective on certain tasks that are near impossible for the state-of-the-art \acrshort{acr:llm} of \acrshort{acr:gpt}-4, particularly in the mathematical reasoning challenge called \enquote{Game of 24}.

\cite{zhouLanguageAgentTree2023} introduces a framework called \gls{acr:lats} "that synergizes the capabilities of \acrshortpl{acr:llm} in planning, acting, and reasoning.". As of writing (October 30th, 2023), the \gls{acr:lats} framework is the highest scoring model on the HumanEval benchmark (see \autoref{subsec:benchmarks}), demonstrating state-of-the-art performance on decision-making tasks in a range of diverse domains. \gls{acr:lats} performs a sequence of operations in succession until the task at hand is solved. These are \textit{selection, expansion, evaluation, simulation, backpropagation, and reflection}. Employing Monte Carlo Tree Search they enable the \acrshort{acr:llm}-based to select the among $n$ sampled options while still exploring other promising alternatives, using a heuristic to rank alternatives. Though a shared space of thoughts and actions, the framework supports both reasoning and decision-making tasks. Observation and self-reflection abilities enables \acrshort{acr:lats} to use external feedback, which proved valuable when testing the framework on different benchmarks, some of which were discussed in \autoref{subsec:benchmarks}.



\section[Retrieval Augmented Generation]{\acrlong{acr:rag}}\label{sec:retrieval-automented-generation}

\gls{acr:rag} is tightly interwoven with explainable \acrshort{acr:ai}, being a framework for retrieving facts from an external knowledge base to allow a \acrshort{acr:llm}-based agent access to accurate up-to-date information \citep{martineauWhatRetrievalaugmentedGeneration2023}. A common problem when working with language models, especially those designed to be general-purpose, is hallucination; that is, when the model provides an answer that is completely wrong but in a very convincing manner. While progress is being made with newer models even the better ones, like GPT-4, gives an incorrect answer about 1 out 5 times, and even worse for certain categories of queries (for instance 'code' and 'business') \citep[10]{openaiGPT4TechnicalReport2023}. \acrlong{acr:rag} can help mitigate this problem.

\subsection{LangChain}\label{subsubsec:langchain}

LangChain \citep{chaseLangChain2022} is an open-source project that provides tooling that can be used to create autonomous \acrshort{acr:ai} agents. It is designed to help with prompt management and optimization, creating chains of calls to \acrshortpl{acr:llm}, data augmented generation, autonomous agent creation, and memory-related tasks.

\cite{nascimentoFamilyNaturalLanguage} experimented with using ChatGPT with LangChain for \glspl{acr:nlidb}, that is, allowing the querier of a database to use natural language queries such as \enquote{Give me locations of all churches in Trondheim along with a short description} instead of \acrshort{acr:sql} queries. They saw promising results when using \texttt{SQLDatabaseChain}, which inspects database schemas, tables, and joins in the database one provides it with. Doing so also helps mitigate issues with exceeding the ChatGPT token limit, compared with passing entire schemas as prefaces to the prompt itself. However, while the method was able to answer 13/27 test queries correct, using keyword search tools along with ChatGPT proved significantly more applicable, answering 22 correct and only 5 wrong.

\subsection{AutoGPT}\label{subsubsec:autogpt}

\cite{richardAutoGPTHeartOpensource2023} will try to split a task into subtasks and use the internet and other tools in an automatic loop to solve the task/subtasks. AutoGPT comes with ready-to-go code templates for various purposes, benchmarks for agent performance measurements, and \acrshort{acr:ui} and \acrshort{acr:cli} tools to control and monitor agents. The AutoGPT project adopts the  \textit{Agent Protocol} \cite{AgentProtocol}, which is an OpenAPI specification v3 based protocol that provides a common interface for communicating with agents. This ensures compatibility with future applications, and is currently used for communication with the \acrshort{acr:ui} and \acrshort{acr:cli} tools.

\cite{firatWhatIfGPT42023} performed an exploratory study to map different use cases and experiences of AutoGPT users. They found that content creation, such as making a podcast outline, is a common use case for AutoGPT-powered applications. Other applications include data summarization and information organization. The authors highlight limitations token limit and inefficiency. AutoGPT is known to have behave unreliable at times, and a common complaint is that it gets stuck in \enquote{reasoning loops}\footnote{\url{https://github.com/Significant-Gravitas/AutoGPT/discussions/1939}}\footnote{\url{https://github.com/Significant-Gravitas/AutoGPT/issues/1994}}.

\subsection{AutoGen and Microsoft Semantic Kernel}\label{subsubsec:microsoft-semantic-kernel}

AutoGen and the Microsoft Semantic Kernel are both Microsoft aimed at creating autonomous \acrshort{acr:ai}-based agents. AutoGen \cite{wuAutoGenEnablingNextGen2023} is a generic framework that allows for multi-agent applications in which agents can converse with each other. The authors demonstrate the effectiveness of the approach in domains including mathematics, coding, and online decision-making. They highlight improved performance, reduced development code, and decreased manual burden for existing applications as the main benefits. It also allows for limiting of fixed back-and-forth interactions between the \acrshort{acr:ai} agent and the human user by allowing

Microsoft Semantic Kernel\footnote{\url{https://github.com/microsoft/semantic-kernel}} is an \acrshort{acr:sdk} that functions as the brain of an autonomous agent and provides connectors to models and memory, and connects to triggers and actions. \cite{maedaAutoGenAgentsMeet2023} talks about how Semantic Kernel can be used to augment the abilities of AutoGen agents by providing it with hooks into the real world. These \textit{hooks} can be native functions that written by the developer, or existing OpenAI/Semantic Kernel plugins, like the WebPages Plugin which fetches a given URL and returns the text found.

\glsresetall